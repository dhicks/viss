---
title: "Developing Measures of Public Perceptions of Values in Science"
author: 
    - name: Daniel J. Hicks
      corresponding: true
      orcid: 0000-0001-7945-4416
      email: dhicks4@ucmerced.edu
      affiliations:
          - id: ucm
            name: University of California, Merced
            address: 5200 N Lake Road
            city: Merced
            state: CA
            postal-code: 95343
    - name: Emilio Lobato
      orcid: 0000-0002-3066-2932
      email: elobato@ucmerced.edu
      affiliations: 
        - ref: ucm
    - name: Joseph Dad
      affiliations:
        - ref: ucm
    - name: Cosmo Campbell
      affiliations:
        - ref: ucm

abstract: "abstract"

keywords: [keyword1, keyword2, keyword3]

format:
    apaquarto-html: default
    apaquarto-docx: default
    apaquarto-pdf: default
    apaquarto-typst: default
---

<!--
SCX submission guidelines: <https://journals.sagepub.com/author-instructions/scx>
7000-9000 words, including references
(a) a separate title page file with the names, addresses, telephone numbers, and e-mails of all authors; (b) a separate biography page file of 50 to 80 words for biographical descriptions of each author; and (b) an abstract of not more than 100 words accompanied by approximately 4-5 suggested keywords

tables at end of MS, figures in separate files

anonymous review

cannot accept PDFs(?)



PUS submission guidelines: <https://journals.sagepub.com/author-instructions/PUS>
Research Article (max. 8,000 words and 5 figures or tables)
Research Note (max. 4,000 words and 5 figures or tables)

OA agreement: library covers first $1k, 20% off remaining APC
-->

```{r}
#| label: setup
#| include: false
library(tidyverse)
library(knitr)
```


# Introduction #

Historians, philosophers, and sociologists of science (HPSTS scholars) who study public scientific controversies often put forward explanations for these controversies that refer to views of publics on the relationship (actual or normative) between science, values, and policy.  *[examples]*

However, in the citations above, there is little empirical evidence to support key claims about the views of publics.  Historians are carefully tracking the activities of scientists or industry actors, and assume, for example, that public acceptance of climate science or policy depends on perceptions of a scientific consensus *[Oreskes]*.  The sociologists cited above are providing rich accounts of particular cases, rooted in ethnographic fieldwork.  But these cases don't obviously generalize to other cases or the general public.  And, at best, philosophers are candid about offering speculation and empirical research questions.  *[notable exceptions are Weisberg et al. and Elliott et al.]*

The aim of the current study is to develop a Values in Science Scale (VISS) that could be used to measure public views on the relationship between science, values, and policy.  

*[roadmap]*

## Software and reproducibility

<!--
Data cleaning and analysis was conducted in R version 4.1.2 [@RCoreTeamLanguageEnvironmentStatistical2021], with extensive use of the `tidyverse` suite of packages version 1.3.1 [@WickhamWelcomeTidyverse2019].  Regression tables were generated using the packages `gt` version 0.5.0 [@IannoneGtEasilyCreate2022] and `gtsummary` version 1.6.0 [@SjobergReproducibleSummaryTables2021]. 

Anonymized original data and reproducible code are available at <https://github.com/dhicks/transparency>.  Instructions in that repository explain how to automatically reproduce our analysis. 
 -->



# Study 1

## Materials

### Values in Science Scale Items

Draft items for the Values in Science Scale (VISS items) were prepared by [author 1], a philosopher of science specializing in these issues.  [author 1] reviewed the instrument used by *[Weisberg et al.]* along with the text of *[Kovaka]* to create an initial list of items.  These items were grouped into "areas," corresponding to views and ideas discussed in the philosophy of science literature.  Each area was given 3 items.  Additional areas were added, with items written by [author 1], to cover a wide range of views and ideas from the philosophical literature.  Importantly, we did not hypothesize that responses within each area would be correlated.  On the contrary, we expected that the views of the general public would not line up with the positions defended or critiqued by philosophers.  

Each item was a descriptive sentence of approximately 15-20 words, and participants were asked to agree or disagree with each item on a 5-point Likert scale. *[do better words]*

A total of 36 items, for 12 areas, were used in the current study.  These items, their areas, and short labels used in reporting results, are shown in table *[x]*.  *[could walk through the areas here, but let's see how much space we have first]*


### Participants

Data was collected concurrently with a closely related project [redacted <!-- @HicksValuesDisclosuresTrust2022 -->].  Participants were recruited using the online survey platform Prolific, and the survey was administered in a web browser using Qualtrics.  Prolific has an option to draw samples that are balanced to be representative by age, binary gender, and a 5-category race variable (taking values Asian, Black, Mixed, Other, and White) for US adults [@RepresentativeSamplesFAQ2022].  A recent analysis finds that Prolific produces substantially higher quality data than Amazon Mechanical Turk for online survey studies, though three of the five authors are affiliated with Prolific [@PeerDataQualityPlatforms2021].  


*[more]*

The study was approved by the UC Merced IRB on August 17, 2021, and data collection ran October 18-20, 2021.  


## Data

## Analysis

Analysis was conducted in R 4.1 *[cite]* with extensive use of the `tidyverse` suite of R packages *[cite]*.  A cleaned, anonymized dataset and cleaning and analysis scripts are available at *[github repo]*. 



## Results

Despite the efforts undertaken by Prolific to provide a representative sample, exploratory data analysis indicated that our data are unlikely to be representative by education level and political ideology.  In 2021, about 9% of US adults 25 or older had a less than high school education, and 38% had a Bachelor's degree or higher [@CPSHistoricalTime2022 fig. 2].  Only 1% of our participants reported a less than high school education, and 57% reported a Bachelor's degree or higher.  For political ideology, the General Social Survey has consistently found over several decades that about 30% of US adults identify as liberal, about 30% identify as conservative, and about 40% as moderate [@GSSDataExplorer2022].  Among our participants, liberals (574) heavily outnumber conservatives (248; *[<!-- @fig-part-values -->]*).  Both overrepresentation of college graduates and underrepresentation of conservatives (especially conservatives with strong anti-institutional views) are known issues in public opinion polling [@KennedyEvaluation2016Election2018].  

In particular, because political partisanship plays a significant role in many prominent public scientific controversies [though not all\; @FunkAmericansPoliticsScience2015], we believe it will be important to validate a VISS across the political spectrum before using it to understand the dynamics of a controversy.  


### Descriptive analysis


### Factor analysis

To examine the underlying dimensionality of the Values in Science scale, we split the sample in half for the purposes of carrying out exploratory factor analysis (EFA) to see what underlying factor structure may be present based on participants' response and confirmatory factor analysis (CFA) to compare models that could be abstracted from the EFA. We created a dummy variable that randomly assigned participants to one of two groups, resulting in two sub-samples of 439 and 467 response sets, respectively. Before conducting the EFA, we checked whether the assumptions necessary for a valid EFA held. Bartlett's test of sphericity was significant, $$\chi^2$$ (`bartlett$parameter`) = `bartlett$statistic`, p = `bartlett$p.value(round = 3)`. The Kaiser-Meyer-Olkin measure of sampling adequacy was `kmo$MSA(round =2)`. The determinant of the correlation matrix was `det(round = 5)`, indicating no multicollinearity issues.

We conducted our factor analysis on the first sub-sample using the psych package (version #whatever) in R. Parallel analysis indicated we should retain six factors for an exploratory factor analysis, although only three factors had eigenvalues greater than 1.0. As such, we computed solutions for a three and a six factor solution using varimax rotation, deciding to retain items that had at least |.3| factor loading on only one of the resulting factors. The six-factor solution explained 33% of the variance and was preferred because the resulting factor structure was more easily interpretable than the three factor solution, which only explained 25% of the variance.

We retained 29 out of the original 36 items. The retained items, factor loadings, and communalities are all shown in Table #. The first identified factor contains six items that cohere around scientistic perceptions of science, elevating the status of science as superior to other knowledge-production efforts. The second factor identified contains three items that emphasize the role of science within society, focusing on societal consequences of scientific endeavors. The third factor contains eight items that, together, appear to represent participants' level of cynicism regarding the credibility of scientists and the scientific community. The fourth factor contains three items that tap into participants' perceptions of the social power dynamics within the scientific community. The fifth factor identified contains five items that reflect perceptions of science as constrained to a relatively narrow set of methods and practices in order to be counted as science, i.e. a mythological "the" scientific method. The final factor identified contains four items that appear to tap into perceptions of science as capable of pure objectivity.

For cross-validation purposes, we ran a CFA on the second sub-sample using the factor structure extracted from the EFA. We carried out the CFA with maximum likelihood estimation using the lavaan package (version #whatever) in R. Fit indices, unfortunately, revealed the model was insufficient to adequately fit the data, $$\chi^2$$(308) = 981.10, p < .001, CFI = .622, AGFI = .817, RMSEA = .071, and SRMR = .086. Typically, values exceeding .90 for CFI and AGFI indicate acceptable fit and values below .05 for RMSEA and SRMR indicate acceptable fit (Hu & Bentler, 1999).

Despite results from the CFA, the strong theoretical coherence of the factor structure suggests ways in which future iterations of a scale to tap into peoples' perceived values about science could improve upon the current Values in Science scale. More precise item wording may be necessary to better capture elements of the aforementioned constructs. Similarly, additional items may be needed to capture more aspects of each the proposed latent constructs. Lastly, there may be additional relevant values and perceptions about science that are not captured at all in the present iteration of the Values in Science scale. Future work should consider each of these suggestions to build upon the preliminary work we report here on the development of a psychometrically valid instrument to capture peoples' values in science. Excepting the comparative fit index (CFI), the fit indices were close to thresholds indicative of acceptable model fit. As such, we have decided to continue with our subsequent analyses using the six-factor structure as a means of examining whether participant responses to the Values in Science scale has any potential predictive validity for participant responses to the perceived credibility of scientists who acknowledge the role of values in their scientific research. Results of any such tests must be interpreted with caution, however, in light of the CFA results.



# Study 2
{{< include "study 2.md" >}}


# Study 3

The aim of this study was to replicate and extend the relevant components of Study 1 using the finalized VISS items.  

## Materials

After giving informed consent, participants were first asked a commitment question: "We care about the quality of our survey data. For us to get the most accurate measures of your opinions, it is important that you provide thoughtful answers to each question.  Do you commit to providing thoughtful answers to the questions on this survey?"  Responses to this question were not used to include or exclude participants. 

Participants were then presented with five blocks of "perceptions of science" questions, including the finalized VISS items, a subset of the OSI 2.0 *[cites]*, and three blocks with measures of generalized trust in science (i.e., trust in science generally, rather than trust in particular scientists or scientific institutions): the Credibility of Science Scale *[COSS; ref]*, *[scientism scale]*, and a block of ad hoc questions: A question from *[Pew]*, "Overall, science has had a(n) ____ effect on society," with options "mostly negative," "equally positive and negative," and "mostly negative." And variations on the General Social Survey's "confidence in institutions" questions: "Here are some influential institutions in this country. As far as the people running these institutions are concerned, how much confidence would you say that you have in them?"  The response scale had five values, ranging from "None at all" to "A great deal"; and the institutions listed were Colleges and Universities; Congress; Elementary, Middle, and High Schools; Major Companies; Medicine; and Scientific Community.  We considered including a scale designed to test McCright and Dunlap's "anti-reflexivity" thesis [@McCrightInfluencePoliticalIdeology2013], but this was dropped due to survey length/expense. 

Questions within each "perceptions of science" block were presented in random order, and the blocks themselves were presented in random order. 

After completing all of the "perceptions of science" blocks, participants were then presented with two blocks of background/demographic questions.  One block used the reduced (18-item) form of the Right Wing Authoritarianism or Authoritarianism-Conservatism-Traditionalism (ACT) scale [@DuckittTripartiteApproachRightWing2010], and the other block contained other demographics: gender identity and lived gender [questions 2 and 3 of the Multidimensional Sex/Gender Measure (MSGM)\; @BauerTransgenderinclusiveMeasuresSex2017], race-ethnicity, religious affiliation and frequency of attendance, an occupation question designed to measure class as occupational prestige [@HughesOccupationalPrestigeStatus2024], and an 8-step political spectrum scale (3 liberal, 1 centrist, 3 conservative, and a write-in option).  Questions were presented in random order for the RWA block but not the other demographics block.  We considered also including the 7th version of the Social Dominance Orientation scale [SDO~7~\; @HoNatureSocialDominance2015], but this was dropped due to survey length/expense. 

This study was to determined to be exempt from IRB review under Category 2.(i) by DJH, and self-exempt determination was acknowledged by the UC Merced IRB on April 11, 2024 (protocol #UCM2024-46). 


## Data

Data collection was conducted on April 16, 2024.  Participants were recruited on Prolific, limited to US adults and with a "representative" sample based on binary sex, age, and trichotomous political affiliation *[values]*.  We aimed to collect 501 responses.  After removing incomplete responses, our dataset comprised 502 responses.  


## Analysis

Analysis was conducted in R 4.1 *[cite]* with extensive use of the `tidyverse` suite of R packages *[cite]*.  A cleaned, anonymized dataset and cleaning and analysis scripts are available at *[github repo]*. 

## Results

Exploratory data analysis indicated high completion rates (98% or greater) for almost all items on the survey.  The one exception was the measure of occupational prestige [@HughesOccupationalPrestigeStatus2024], which had a completion rate of only 68%.  Further investigation found that these missing values corresponded to "job titles" such as student, unemployed, retired, and homemaker, i.e., respondents who were outside of the labor force.  The seasonally adjusted civil labor force participation rate in the US was 63% in March 2024 (<https://web.archive.org/web/20240703202237/https://www.bls.gov/charts/employment-situation/civilian-labor-force-participation-rate.htm>), and so this sample slightly underrepresented people outside the workforce.  Because of high missingness, we generally did not include the occupational prestige measure in our analysis.  

### Individual item analysis

@fig-03-likert shows the distribution of responses to the VISS items, in descending order by total agreement.  We use 80%/20% of total agreement/disagreement as thresholds for a "near consensus" among respondents.  In study 3, this near consensus appears for four items:  near consensus agreement that the primary aims of science include improving knowledge (aims.1) and understanding threats to human health and the environment (aims.2); and near consensus disagreement that scientists never disagree with each other (consensus.2) and that scientific theories never change (fallible.1).  There is majority agreement with another six VISS items (coi.2, stdpt, vfi.1, ir, nonsubj.1, and nonsubj.2), and majority disagreement with another five (consensus.1, fallible.2, vfi.2, pluralism.1, and value.conflict).  For the idea that scientists use the same strict requirements for evaluating hypotheses (pluralism.3), total disagreement is just shy of a the majority.  

![Distribution of responses to VISS items, ordered by total agreement.  Vertical dashed lines indicate 20%, 50%, and 80%.](img/02_likert.png){#fig-03-likert}

#### The value-free ideal

*[intro]*

VISS contains four items that correspond to the value-free ideal — nonsubj.1 and .2 and vfi.1 and .2.  It also contains four items corresponding to philosophical critiques of this ideal.  aims.2 corresponds to the "aims approach" to values in science, which argues that science has non-epistemic aims — such as protecting human health and the environment — and that such aims can legitimate influence all aspects of research *[cite]*.  ir corresponds to the argument from inductive risk.  According to this argument, because and insofar as science has the potential to cause harmful social consequences, it's appropriate to require greater evidence (or allow weaker evidence) based on the severity of those consequences *[cites]*.  stdpt corresponds to standpoint epistemology, a theory associated with feminist, Marxist, and critical race analyses of science *[cites]*.  According to standpoint epistemology, members of marginalized groups can be in a position to better identify and understand oppressive social phenomena than members of privileged groups.  Finally, value.conflict corresponds to an unusual convergence in two philosophical debates, one in feminist philosophy of science and the other in the relationship between science and religion.  Given that oppressive values (such as misogyny and androcentrism) have and continue to influence some lines of scientific research, some feminist philosophers — though certainly not all — argue that these values provide a sufficient grounds for feminists to question or reject the claims coming out of this research *[cites]*.  @PlantingaWhenFaithReason2001 defends a similar position in the context of debates over religion and evolution, arguing that "There is ... a battle between the Christian community and the forces of unbelief," "that important cultural forces such as science are not neutral with respect to this conflict," and so "we Christians must think about the matter at hand from a Christian perspective; we need Theistic Science" (30).  

@fig-vfi shows marginal distributions for these VFI and critiques, along with scatterplots and correlation coefficients for each VFI-critique pair.  *[marginals first, w/ xref to Likert plot: majority agreement w/ aims.2, ir, stdpt, both nonsubj, vfi.1; some rejection of value.conflict; majority rejection of vfi.2]*

While one would expect pairs of versions of VFI and critiques to be fairly strongly negatively correlated — increased acceptance of a critique would be associated with decreased acceptance of VFI — all of these correlations are near 0.  Two *[3: vfi.2 + stdpt]* pairs do reject the null hypothesis that $R \geq 0$, vfi.1 and ir (R = -0.10, p = 0.01) and nonsubj.1 and stdpt (R = -0.14, p = 0.001).  However, 33% of respondents agree (response > 4) with both vfi and ir; and 35% agree with both nonsubj.1 and stdpt.  In the discussion, we offer an interpretation of these flat correlations in terms of Deweyan "habits of thought" rather than logically inconsistent beliefs.  

![Marginal distributions and scatterplots for VFI items (rows, y-axes of scatterplots) and critiques of VFI (columns, x-axes of scatterplots).  Lines in each scatterplot are OLS regressions with 95% CIs.  Response scales are represented numerically, with 1 = strongly disagree, 7 = strongly agree.  Point positions in scatterplots are jittered for legibility.  P-values are calculated for the null that $R  \geq 0$, not adjusted for multiple comparisons.](img/02_vfi.png){#fig-vfi}

HPSTS scholars, science communicators, and scientists themselves have often claimed that the value-free ideal provides a foundation for trust in science *[cites]*.  The measures of generalized trust in science that we used here were all *[strongly]* correlated with each other; here we report results for COSS *[why]*.  @fig-trust shows scatterplots and correlation analyses between, on the one hand, the four versions of VFI and the four critiques of VFI, and on the other, COSS.  The correlations for nonsubj.2 and vfi.1 were not statistically significantly different from zero (R = -0.03, p = 0.5 for both); while correlations for nonsubj.1 and vfi.2 were significantly different from zero but were negative rather than positive (nonsubj.1: R = -0.2, p = $3 \times 10^-6$ for the point null $R = 0$; vfi.2: R = -0.2, p = $2 \times 10^-7$).  Without adjusting for multiple comparisons, all correlations for the critiques of VFI were significantly different from zero; correlations were positive for aims.2 (R = 0.2, p = $3 \times 10^-4$), stdpt (R = 0.2, p = $4 \times 10^-4$), and value.conflict (R = 0.1, p = 0.02), but negative for ir (R = -0.3, p = $4 \times 10^-10$). 

![Marginal distribution of COSS (rightmost panels) and scatterplots for VFI items (top row) and critiques of VFI (bottom row) (columns, x-axes of scatterplots) against COSS scores (y-axes). Lines in each scatterplot are OLS regressions with 95% CIs.  Response scales are represented numerically, with 1 = strongly disagree, 7 = strongly agree. Point positions in scatterplots are jittered for legibility. P-values are calculated for the null that $R = 0$, not adjusted for multiple comparisons.](img/02_trust.png){#fig-trust}
 

### Factor analysis




# General Discussion




*[Deweyan "habits of thought": vfi and crits; VFI + crits and trust]*

The patterns of correlation in @fig-trust challenge the common idea that VFI provides a foundation for trust in science.  Our findings indicate that trust in science is either independent of or *negatively* associated with VFI.   *[Further, in the study 3 EFA, the four versions of VFI and wait.policy consistently comprise the "objectivity" latent variable; and this variable is consistently uncorrelated with trust.]*  
