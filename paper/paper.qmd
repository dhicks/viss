---
title: "Developing Measures of Public Perceptions of Values in Science"
author: 
    - name: Daniel J. Hicks
      corresponding: true
      orcid: 0000-0001-7945-4416
      email: dhicks4@ucmerced.edu
      affiliations:
          - id: ucm
            name: University of California, Merced
            address: 5200 N Lake Road
            city: Merced
            state: CA
            postal-code: 95343
    - name: Emilio Lobato
      orcid: 0000-0002-3066-2932
      email: elobato@ucmerced.edu
      affiliations: 
        - ref: ucm
    - name: Joseph Dad
      affiliations:
        - ref: ucm
    - name: Cosmo Campbell
      affiliations:
        - ref: ucm

abstract: "abstract"

keywords: [keyword1, keyword2, keyword3]

format:
    apaquarto-html: default
    apaquarto-docx: default
    apaquarto-pdf: 
        keep-tex: false
        pdf-engine: xelatex
    apaquarto-typst: default
    
bibliography: VISS.yaml
---

<!--
SCX submission guidelines: <https://journals.sagepub.com/author-instructions/scx>
7000-9000 words, including references
(a) a separate title page file with the names, addresses, telephone numbers, and e-mails of all authors; (b) a separate biography page file of 50 to 80 words for biographical descriptions of each author; and (b) an abstract of not more than 100 words accompanied by approximately 4-5 suggested keywords

tables at end of MS, figures in separate files

anonymous review

cannot accept PDFs(?)



PUS submission guidelines: <https://journals.sagepub.com/author-instructions/PUS>
Research Article (max. 8,000 words and 5 figures or tables)
Research Note (max. 4,000 words and 5 figures or tables)

OA agreement: library covers first $1k, 20% off remaining APC
-->

```{r}
#| label: setup
#| include: false
library(tidyverse)
library(knitr)
library(gt)
library(here)
```

# Introduction #

Historians, philosophers, and sociologists of science (HPSTS scholars) who study public scientific controversies often put forward explanations for these controversies that refer to views of publics on the relationship (actual or normative) between science, values, and policy.  *[examples]*

However, in the citations above, there is little empirical evidence to support key claims about the views of publics.  Historians are carefully tracking the activities of scientists or industry actors, and assume, for example, that public acceptance of climate science or policy depends on perceptions of a scientific consensus *[Oreskes]*.  The sociologists cited above are providing rich accounts of particular cases, rooted in ethnographic fieldwork.  But these cases don't obviously generalize to other cases or the general public.  And, at best, philosophers are candid about offering speculation and empirical research questions.  *[notable exceptions are Weisberg et al. and Elliott et al.]*

The aim of the current study is to develop a Values in Science Scale (VISS) that could be used to measure public views on the relationship between science, values, and policy.  

*[roadmap]*

# Background and initial development of VISS items

In the twentieth century, philosophy was understood as a purely non-empirical discipline, strictly separated from history, sociology, and psychology *[Reichenbach]* and using either formal methods (logic, probability, game theory) or "armchair" reflection on language.  However, starting in the 1970s, philosophers of science began to engage more closely with sibling empirical fields, giving rise to approaches such as philosophy of science in practice — which often uses ethnographic methods *[cite]* — and integrated history and philosophy of science, based on the conviction that "the common goal of understanding of science can be pursued by dual, interdependent means" *[https://integratedhps.org/about.html]*.  

Perhaps for this reason, contemporary philosophers of science are not shy about basing their analysis on empirical claims about public views of normative relationship between science and society.  For example, philosophers of science have argued that public scientific controversies (e.g., over climate change or vaccinations) can arise, in part, from a mismatch between the widely-held ideal of value-free science (often abbreviated as VFI) and the reality that science is necessarily value-laden [@HavstadNeutralityRelevancePrescription2017; @JohnEpistemicTrustEthics2017; @FernandezPintoLegitimizingValuesRegulatory2019; @KovakaClimateChangeDenial2021; @HolmanNewDemarcationProblem2022].  On the one hand, scientists, policymakers, and members of the general public all (are assumed to) endorse the idea that social, political, and ethical values should not influence the evaluation and acceptance of hypotheses (the so-called "core" of scientific research).  On the other hand, decades of empirical work in history, philosophy, and social studies of science has shown not only that such values have exactly this kind of influence, but that that values have essential and positive roles to play even in the evaluation and acceptance of hypotheses *[cites]*.  "Merchants of doubt" [@OreskesScientificConsensusClimate2004; @FernandezPintoKnowBetterNot2017] exploit this mismatch, pointing to actual influences of values as a way to undermine trust or confidence in scientific findings.  @FernandezPintoLegitimizingValuesRegulatory2019 argue that, in this way, VFI is "self-undermining." 

The scholarly literature on public scientific controversies includes similar appeals to public ideas about consensus and dissent [@OreskesScientificConsensusClimate2004; @CookQuantifyingConsensusAnthropogenic2013; @JohnEpistemicTrustEthics2017; @MyersConsensusRevisitedQuantifying2021]; concerns about paternalism, technocracy, or elitism [@WynneSheepfarmingChernobylCase1989; @JordanTrustworthinessResearchParadigm2011; @LargentVaccineDebateModern2012; @NavinValuesVaccineRefusal2015; @GoldenbergVaccineHesitancyPublic2021]; conflicts of interest, manufacturing doubt or certainty [@ElliottSciencePolicyTransparency2014; @FernandezPintoCommercialInterestsErosion2020; @GoldenbergVaccineHesitancyPublic2021]; and science's historical and/or contemporary role in oppression [@SchemanEpistemologyResuscitatedObjectivity2001; @WashingtonMedicalApartheidDark2008; @McHughMoreSkinDeep2011; @NelsonBodySoulBlack2011; @GrasswickUnderstandingEpistemicTrust2018;  @GoldenbergVaccineHesitancyPublic2021].  Like appeals to the VFI mismatch, these claims are generally made with little or no evidence that members of the public actually hold the views in question.  

There is a substantial body of empirical research on scientific literacy and understanding of the nature of science (NOS) among the general public.  However, the instruments used in this research generally focus on "science facts" (e.g., "Electrons are smaller than atoms. True or false?"), scientific reasoning ("Imagine that we roll a fair, six-sided die
1000 times. Out of 1000 rolls, how many times do you think the die would come up as an even number?"), or the epistemology of science ("Scientific theories are subject to ongoing testing and revision") [examples 1 and 2 from @KahanOrdinaryScienceIntelligence2017\; example 3 from @WeisbergKnowledgeNatureScience2020], with much less attention to the relationship between science and society or the role of values in science.  For example, @WeisbergKnowledgeNatureScience2020 — a collaboration which included two philosophers of science — use a NOS instrument to study predictors of attitudes to evolution.  In the preregistration document (<https://osf.io/dncj4>), this instrument is characterized more specifically, as measuring participant views on the "Epistemology of Science," and none of the 23 items in the instrument cover the relationship between science and society or values in science.  

To our knowledge, the only empirical studies of public views on VFI are @ElliottValuesEnvironmentalResearch2017 and @HicksValuesDisclosuresTrust2022, the latter of which is a replication and re-analysis of the former.  In an online survey experiment, @HicksValuesDisclosuresTrust2022 find that disclosing public health values might increase, rather than decrease, trust in the disclosing values.  

The current project aimed to fill the gap by developing a survey instrument to measure views of values and science among the general public: the Values in Science Scale (VISS).  

Initial development of the list was conducted by [author1], a philosopher of science, who assembled 3 prompts based on each of 12 different major issues or areas of research in the philosophical literature.  We emphasize that these prompts are theoretically-informed but were not developed to capture *a priori* latent variables.  For example the three "aims of science" prompts state that the aims of science are knowledge for its own sake, the protection of human health, and economic growth.  We do not expect responses on these items to be strongly correlated, and so do not expect exploratory factor analysis to indicate that they form a "aims of science" latent variable.  Some prompts were based on other instruments *[Toolbox, Weisberg et al. NOS scale]* or quotations from scholarly sources; others were written by [author1] to capture a key idea.  

The first two columns of @tbl-viss contain the original items from the first version of VISS. Each item was a declarative sentence of approximately 15-20 words, and participants were asked to agree or disagree with each item on a 5-point Likert scale.  

We briefly review the relevant philosophy of science literature for each of the 12 issues/areas.  

Aims of science
: The so-called "aims approach" proposes, as an alternative to the value-free ideal, that the non-epistemic aims of science can legitimately influence the evaluation and acceptance of hypotheses.  [@ElliottNonepistemicValuesMultiple2014; @HicksNewDirectionScience2014; @IntemannDistinguishingLegitimateIllegitimate2015; @PotochnikDiverseAimsScience2015]

Conflicts of interest
: Across numerous controversies, scholars and other analysts have identified and criticized the influence of commercial interests on both academic and government research: tobacco [@ProctorGoldenHolocaustOrigins2012], climate [@OreskesMerchantsDoubtHow2011], pesticides [@DurantIgnoranceLoopsHow2020], chemical safety [@CranorHowLawPromotes2020], pharmaceuticals [@LexchinThoseWhoHave2011], workplace safety [@MichaelsDoubtTheirProduct2008].  Several philosophers of science have studied the effects of conflicts of industry and preferential industry funding on the scientific community as a whole using agent-based models [@HolmanPromisePerilsIndustryfunded2018; @OConnorMisinformationAgeHow2019; @PintoEpistemicDiversityIndustrial2023].  

Consensus
: Especially in the context of climate change, several authors have observed that climate skeptics have appealed to "Galileo narratives" and otherwise rejected the idea of a consensus as anti-scientific [@BiddleClimateSkepticismManufacture2015; @OreskesResponseOreskesCounting2017; @KovakaClimateChangeDenial2021].  

Facts vs. values
: Empiricists have traditionally followed David Hume in making a sharp distinction between "facts" and "values."  Values are often taken to be non-cognitive, that is, logically independent from evidence and reason.  Philosophers of science in the pragmatist, feminist, and Marxist traditions have challenged the fact/value distinction, and argued that it leads to inappropriately treating (supposedly non-cognitive) values as polluting or corrupting science, that is, the value-free ideal [@AndersonUsesValueJudgments2004; @BrownScienceMoralImagination2020; @CloughUsingValuesEvidence2020].  

Fallibilism
: Historians of science have noted that concerns about demarcating science from pseudo-science arose in connection with both the expansion of science's social authority and fallibilism, the idea that knowledge is never certain [@GierynBoundaryWorkDemarcationScience1983; @LaudanDemiseDemarcationProblem1983].  Uncertainty is a significant factor in questions about scientific authority in the climate controversy [@BrownDisconnectProblemScientific2016] and the argument from inductive risk starts with the recognition that empirical claims are never certain.  

Inductive risk
: The argument from inductive risk challenges the value-free ideal as follows:  scientists should anticipate the non-epistemic downstream consequences of error (eg, accepting a false hypothesis); this requires appealing to social, political, and ethical values; and so these values should influence the core of inquiry [@DouglasSciencePolicyValuefree2009].  This argument has been enormously influential in philosophy of science over the last decade+, transforming science, values, and policy from a marginal topic into a major one.  The prompts here include a general and specific version of the idea of inductive risk, along with a prompt on the social responsibilities of scientists that @DouglasSciencePolicyValuefree2009 examines in depth.  

Non-subjectivity
: "Objectivity" is highly polysemous [@LloydObjectivityDoubleStandard1995; @DouglasSciencePolicyValuefree2009 ch. 6].  Douglas defines "value-free objectivity" as "all values (or all subjective or “biasing” influences) are banned from the reasoning process" [@DouglasSciencePolicyValuefree2009 122].  We focus on assumptions, speculation, interpretation, and judgment as subjective aspects of inquiry that might be independent of ethical, social, and political values.  

Pluralism
: Students are often still taught the crude model of "the scientific method," and many scientists endorse a simple universalism — the idea that all areas of science use the same method, such as Popper's falsificationism — in their reflections on how science works.  @MercerWhyPopperCan2016 observes both climate scientists and skeptics appealing to Popper in the climate controversy, and Oreskes [@LloydClimateModellingPhilosophical2017 ch. 2] discusses the ways that climate science is an awkward fit for a simple universalist conception of science.  

Scientism
: Scholars in STS have long observed publics reacting against exaggerated claims of scientific expertise [@WynneSheepfarmingChernobylCase1989; @EpsteinImpureScience1996; @FrickelUndoneScienceCharting2010].  This set of questions includes prompts for both "weak" and "strong" scientism [@MizrahiWhatBadScientism2017]. 

Standpoint
: Standpoint epistemology is a longstanding challenge to the value-free ideal, with a history that runs through feminist philosophy of science and critical race theory to Marxist epistemology [@HardingWhoseScienceWhose1991; @WylieComingTermsValues2007; @CrasnowFeministPhilosophyScience2013].  Standpoint is based on two claims, the situated knowledge thesis and idea that marginalized groups are epistemically advantaged.  

Technocracy
: Technocracy is closely related to scientism.  Where scientism is an epistemological view — science is better/the only way of producing *knowledge* — we understand technocracy as a view about *power*.  *[todo]*

Value-free ideal
: These prompts include the value-free ideal, along with another aspect of objectivity — imagination and creativity — and the idea that scientists are especially "objective." *[todo]*







## Software and reproducibility

*[copied from other paper]*
Data cleaning and analysis was conducted in R version 4.1.2 [@RCoreTeamLanguageEnvironmentStatistical2021], with extensive use of the `tidyverse` suite of packages version 1.3.1 [@WickhamWelcomeTidyverse2019].  Regression tables were generated using the packages `gt` version 0.5.0 [@IannoneGtEasilyCreate2022] and `gtsummary` version 1.6.0 [@SjobergReproducibleSummaryTables2021]. 

Anonymized original data and reproducible code are available at <https://github.com/dhicks/transparency>.  Instructions in that repository explain how to automatically reproduce our analysis. 




# Study 1

## Materials

This study used the initial version of the VISS items, as described above and shown in the first two columns of @tbl-viss.  Data was collected concurrently with a closely related project [redacted <!-- @HicksValuesDisclosuresTrust2022 -->].  

After giving informed consent, participants were presented sequentially with the VISS instrument, the experiment described in [redacted <!-- @HicksValuesDisclosuresTrust2022 -->], a subset of the OSI 2.0 [@KahanOrdinaryScienceIntelligence2017], and a series of demographic questions: age, gender identity and lived gender [questions 2 and 3 of the Multidimensional Sex/Gender Measure (MSGM)\; @BauerTransgenderinclusiveMeasuresSex2017], race-ethnicity, religious affiliation and frequency of attendance, an 8-step political spectrum scale (3 liberal, 1 centrist, 3 conservative, and a write-in option), partisan political affiliation, and highest degree (less than high school; high school or some college; Bachelor's degree or higher).  Questions were presented in random order for the VISS instrument and OSI 2.0 subset.  

The study was approved by the UC Merced IRB on August 17, 2021. 


## Data

Data collection was conducted October 18-20, 2021.  Participants were recruited using the online survey platform Prolific, and the survey was administered in a web browser using Qualtrics.  Prolific has an option to draw samples that are balanced to be representative by age, binary gender, and a 5-category race variable (taking values Asian, Black, Mixed, Other, and White) for US adults [@RepresentativeSamplesFAQ2022].  A recent analysis found that Prolific produces substantially higher quality data than Amazon Mechanical Turk for online survey studies, though three of the five authors of that analysis are affiliated with Prolific [@PeerDataQualityPlatforms2021].  We aimed to collect 1000 responses.  After removing incomplete responses, our dataset comprised 988 responses. 

## Analysis

Analysis was conducted in R 4.1 *[cite]* with extensive use of the `tidyverse` suite of R packages *[cite]*.  A cleaned, anonymized dataset and cleaning and analysis scripts are available at *[github repo]*. 



## Results

Despite Prolific claiming to provide a "representative" sample, exploratory data analysis indicated that our data are not representative, based on education level and political ideology.  In 2021, about 9% of US adults 25 or older had a less than high school education, and 38% had a Bachelor's degree or higher [@CPSHistoricalTime2022 fig. 2].  Only 1% of our participants reported a less than high school education, and 57% reported a Bachelor's degree or higher.  For political ideology, the General Social Survey has consistently found over several decades that about 30% of US adults identify as liberal, about 30% identify as conservative, and about 40% as moderate [@GSSDataExplorer2022].  Among our participants, liberals (574) heavily outnumber conservatives (248).  Both overrepresentation of college graduates and underrepresentation of conservatives (especially conservatives with strong anti-institutional views) are known issues in public opinion polling [@KennedyEvaluation2016Election2018].  

In particular, because political partisanship plays a significant role in many prominent public scientific controversies [though not all\; @FunkAmericansPoliticsScience2015], we determined that it would be important to validate a finalized VISS across the political spectrum before putting it to use. 


### Descriptive analysis


### Factor analysis

To examine the underlying dimensionality of the Values in Science scale, we split the sample in half for the purposes of carrying out exploratory factor analysis (EFA) to see what underlying factor structure may be present based on participants' response and confirmatory factor analysis (CFA) to compare models that could be abstracted from the EFA. We created a dummy variable that randomly assigned participants to one of two groups, resulting in two sub-samples of 439 and 467 response sets, respectively. Before conducting the EFA, we checked whether the assumptions necessary for a valid EFA held. Bartlett's test of sphericity was significant, $\chi^2(35) = 307.93, p = 2.20 \times 10^{-45}$. The Kaiser-Meyer-Olkin measure of sampling adequacy was 0.82. The determinant of the correlation matrix was 0.00020, indicating no multicollinearity issues.

We conducted our factor analysis on the first sub-sample using the psych package (version 2.19) in R. Parallel analysis indicated we should retain six factors for an exploratory factor analysis, although only three factors had eigenvalues greater than 1.0. As such, we computed solutions for a three and a six factor solution using varimax rotation, deciding to retain items that had at least |.3| factor loading on only one of the resulting factors. The six-factor solution explained 33% of the variance and was preferred because the resulting factor structure was more easily interpretable than the three factor solution, which only explained 25% of the variance.

This EFA retained 29 out of the original 36 items. The retained items, factor loadings, and communalities are all shown in *[Table #]*. The first identified factor contains six items that cohere around scientistic perceptions of science, elevating the status of science as superior to other knowledge-production efforts.  We labelled this factor *scientism*.  The second factor identified contains three items that emphasize the role of science within society and *values in science*. The third factor contains eight items that, together, appear to represent participants' level of *cynicism* regarding the credibility of scientists and the scientific community. The fourth factor contains three items that tap into participants' perceptions of the social *power* dynamics within the scientific community. The fifth factor identified contains five items that reflect perceptions of science as constrained to a relatively narrow set of methods and practices in order to be counted as science, i.e., a mythological "the" scientific method as presented in a *textbook*. The final factor identified contains four items that appear to tap into perceptions of science in line with the traditional *value-free ideal*. 

For cross-validation purposes, we ran a CFA on the second sub-sample using the factor structure extracted from the EFA. We carried out the CFA with maximum likelihood estimation using the lavaan package (version 0.6-10) in R. Fit indices, unfortunately, revealed the model was insufficient to adequately fit the data, $\chi^2(308) = 981.10, p < .001$, CFI = .622, AGFI = .817, RMSEA = .071, and SRMR = .086. Typically, values exceeding .90 for CFI and AGFI indicate acceptable fit and values below .05 for RMSEA and SRMR indicate acceptable fit (Hu & Bentler, 1999).

<!-- Despite results from the CFA, the strong theoretical coherence of the factor structure suggests ways in which future iterations of a scale to tap into peoples' perceived values about science could improve upon the current Values in Science scale. More precise item wording may be necessary to better capture elements of the aforementioned constructs. Similarly, additional items may be needed to capture more aspects of each the proposed latent constructs. Lastly, there may be additional relevant values and perceptions about science that are not captured at all in the present iteration of the Values in Science scale. Future work should consider each of these suggestions to build upon the preliminary work we report here on the development of a psychometrically valid instrument to capture peoples' values in science. Excepting the comparative fit index (CFI), the fit indices were close to thresholds indicative of acceptable model fit. As such, we have decided to continue with our subsequent analyses using the six-factor structure as a means of examining whether participant responses to the Values in Science scale has any potential predictive validity for participant responses to the perceived credibility of scientists who acknowledge the role of values in their scientific research. Results of any such tests must be interpreted with caution, however, in light of the CFA results.  -->

## Construct validity

Beyond the poor fit statistics in the CFA, qualitative review of the six-factor EFA solution raised concerns about construct validity.  For example, factor 3, *cynicism*, covered a number of items expressing a cynical attitude towards science, with loaded items related to conflicts of interest, the idea that "The consensus of the scientific community is based on social status and prestige rather than evidence," and the idea that "When a scientific theory changes or is revised, it means that the research that went into it initially was flawed."  However, this factor also had the item "Standards of scientific evidence are different in different situations."  We intended this item to articulate the argument from inductive risk [@DouglasSciencePolicyValuefree2009; @HavstadSensationalScienceArchaic2021], that is, the idea that a hypothesis with serious downstream risks if it turns out to be false — such as a claim that a widely-used pesticide does not cause non-Hodgkins lymphoma — should require significantly more evidence than a hypothesis without such downstream risks — such as a claim about the durability of a certain hiking shoe.  

Trying to understand why the argument from inductive risk might be correlated with cynical views of science, we realized that some unknowable fraction of participants might have interpreted the item as something like "scientists inappropriately manipulate standards of evidence to claim support for convenient claims and suppress inconvenient ones."  This indicated a severe threat to construct validity of at least some VISS items.  We also conjectured that construct validity might be creating a kind of measurement error that was attenuating correlations among the VISS items [@PadillaCorrelationAttenuationDue2012] and thereby weakening the CFA fit statistics.  So addressing construct validity might also improve factor analysis fit.  

The research team (at that point comprising [author1] and [author2]) discussed this issue among ourselves and solicited feedback from colleagues at conference presentations of preliminary results.  Based on these discussions, we decided that the next step was a qualitative study to secure construct validity.  


# Study 2

## Materials

[author1] selected a short list of 18 items to focus on moving forward.  This selection was based, in part, on the EFA results from study 1:  items that did not load on to any factors were dropped, as were items that loaded on to the scientism factor.  The scientism factor was dropped in part because of overlap with a scale intended to measure scientism specifically [@LukicDelineatingScientismScience2023]; and because we came to believe that, for the purposes of understanding the dynamics of public scientific controversies, scientism could be reduced to the idea that science is especially trustworthy.  

[author3] and [author4] joined the project in January 2024, and all four authors authors together reviewed this shorter list, making initial revisions based on our impressions of which items seemed to be vague or otherwise difficult to understand.  Partway through the iterative data collection and analysis process, a 19th item was added to the list. 

For this study, items were arranged into 3 blocks of 5-7 items each, roughly corresponding to latent variables identified in the study 1 EFA.  In Qualtrics, we prepared a survey in which participants were randomly allocated to a single block.  Participants were presented with the item text, a 5-point agree/disagree Likert scale (later expanded to a 7-point scale), and a free text response box.  They were instructed to "Please indicate how much you agree with each of the following statements. Then, in the text box, please write a few sentences explaining why you answered the way you did." 

For study 2, besides participant responses to the items, no other data was collected or used.  

This study was to determined to be exempt from IRB review under Category 2.(i) by [author 1], and self-exempt determination was acknowledged by the UC Merced IRB on February 27, 2024 (protocol #UCM2024-30). 

## Data

Data collection and analysis was conducted iteratively, alternating between collection and analysis, until saturation was reached for all items across two consecutive iterations.  Ultimately, 5 iterations were run, all during March 2024.  

In each iteration, participants were recruited on Prolific.  In iterations 1-3, participants had to be adult US residents; no other screeners were used.  We set the target quota for 10 participants per block; except in iteration 3, which had only a single block (a subset of items that had not yet achieved saturation) and for which 15 participants were recruited.  Iterations proceeded until all items had achieved saturation.  Based on study 1, we knew the platform tended to undersample political conservatives and participants with only a high school education.  Because there is significant interest in conservative trust in science — or lack therefore — we wanted to be confident that the VISS would be useful in studying these groups.  We therefore ran two final iterations, one for each of these groups, using all 3 blocks, 10 participants per block, and an additional screener for the target group, to confirm saturation of all items for these subpopulations. 

Across 5 iterations, we collected a total of 875 text responses from 141 participants.  

## Analysis

After the quota was achieved for a given iteration, responses were retrieved from Qualtrics in spreadsheet form and analyzed by all authors, first independently and together in weekly lab group meetings.  We focused on responses that indicated confusion or uncertainty, either explicit or implied, and responses that deviated from the intended meaning of the item.  

If qualitative analysis indicated that an item was frequently misinterpreted or regarded as confusing, all authors worked together to develop alternative wording.  This rewording was then used in future iterations.  An item was considered to have achieved "saturation" when all four authors agreed that the responses across two iterations indicated that it was understood by the participants as intended.  

## Results

The final items and text are given in the third and fourth columns of @tbl-viss. 


# Study 3

The aim of this study was to replicate and extend the relevant components of Study 1 using the finalized VISS items.  Specifically, we were interested in whether and how VISS items might map on to latent variables; *[correlations among items vs. logical relationships; 'upstream' demographic predictors; 'downstream' relationships w/ trust]*

## Materials

After giving informed consent, participants were first asked a commitment question: "We care about the quality of our survey data. For us to get the most accurate measures of your opinions, it is important that you provide thoughtful answers to each question.  Do you commit to providing thoughtful answers to the questions on this survey?"  Responses to this question were not used to include or exclude participants. 

Participants were then presented with five blocks of "perceptions of science" questions, including the finalized VISS items, a subset of the OSI 2.0 [@KahanOrdinaryScienceIntelligence2017], and three blocks with measures of generalized trust in science (i.e., trust in science generally, rather than trust in particular scientists or scientific institutions): the Credibility of Science Scale *[COSS; ref]*, *[scientism scale]*, and a block of ad hoc questions: A question from *[Pew]*, "Overall, science has had a(n) ____ effect on society," with options "mostly negative," "equally positive and negative," and "mostly negative." And variations on the General Social Survey's "confidence in institutions" questions: "Here are some influential institutions in this country. As far as the people running these institutions are concerned, how much confidence would you say that you have in them?"  The response scale had five values, ranging from "None at all" to "A great deal"; and the institutions listed were Colleges and Universities; Congress; Elementary, Middle, and High Schools; Major Companies; Medicine; and Scientific Community.  We considered including a scale designed to test McCright and Dunlap's "anti-reflexivity" thesis [@McCrightInfluencePoliticalIdeology2013], but this was dropped due to survey length/expense. 

Questions within each "perceptions of science" block were presented in random order, and the blocks themselves were presented in random order. 

After completing all of the "perceptions of science" blocks, participants were then presented with two blocks of background/demographic questions.  One block used the reduced (18-item) form of the Right Wing Authoritarianism or Authoritarianism-Conservatism-Traditionalism (ACT) scale [@DuckittTripartiteApproachRightWing2010], and the other block contained other demographics: gender identity and lived gender [questions 2 and 3 of the Multidimensional Sex/Gender Measure (MSGM)\; @BauerTransgenderinclusiveMeasuresSex2017], race-ethnicity, religious affiliation and frequency of attendance, an occupation question designed to measure class as occupational prestige [@HughesOccupationalPrestigeStatus2024], and an 8-step political spectrum scale (3 liberal, 1 centrist, 3 conservative, and a write-in option).  Questions were presented in random order for the RWA block but not the other demographics block.  We considered also including the 7th version of the Social Dominance Orientation scale [SDO~7~\; @HoNatureSocialDominance2015], but this was dropped due to survey length/expense. 

This study was to determined to be exempt from IRB review under Category 2.(i) by DJH, and self-exempt determination was acknowledged by the UC Merced IRB on April 11, 2024 (protocol #UCM2024-46). 


## Data

Data collection was conducted on April 16, 2024.  Participants were recruited on Prolific, limited to US adults and with a "representative" sample based on binary sex, age, and trichotomous political affiliation (Democrat, Republic, or Independent).  We aimed to collect 501 responses.  After removing incomplete responses, our dataset comprised 502 responses.  


## Analysis

Analysis was conducted in R 4.1 *[cite]* with extensive use of the `tidyverse` suite of R packages *[cite]*.  A cleaned, anonymized dataset and cleaning and analysis scripts are available at *[github repo]*. 

## Results

Exploratory data analysis indicated high completion rates (98% or greater) for almost all items on the survey.  The one exception was the measure of occupational prestige [@HughesOccupationalPrestigeStatus2024], which had a completion rate of only 68%.  Further investigation found that these missing values corresponded to "job titles" such as student, unemployed, retired, and homemaker, i.e., respondents who were outside of the labor force.  The seasonally adjusted civil labor force participation rate in the US was 63% in March 2024 (<https://web.archive.org/web/20240703202237/https://www.bls.gov/charts/employment-situation/civilian-labor-force-participation-rate.htm>), and so this sample slightly underrepresented people outside the workforce.  Because of high missingness, we generally did not include the occupational prestige measure in our analysis.  

### Individual item analysis

@fig-03-likert shows the distribution of responses to the VISS items, in descending order by total agreement.  We use 80%/20% of total agreement/disagreement as thresholds for a "near consensus" among respondents.  In study 3, this near consensus appears for four items:  near consensus agreement that the primary aims of science include improving knowledge (aims.1) and understanding threats to human health and the environment (aims.2); and near consensus disagreement that scientists never disagree with each other (consensus.2) and that scientific theories never change (fallible.1).  There is majority agreement with another six VISS items (coi.2, stdpt, vfi.1, ir, nonsubj.1, and nonsubj.2), and majority disagreement with another five (consensus.1, fallible.2, vfi.2, pluralism.1, and value.conflict).  For the idea that scientists use the same strict requirements for evaluating hypotheses (pluralism.3), total disagreement is just shy of a the majority.  

![Distribution of responses to VISS items, ordered by total agreement.  Vertical dashed lines indicate 20%, 50%, and 80%.](img/02_likert.png){#fig-03-likert}

#### The value-free ideal

As discussed in the introduction, the value-free ideal (VFI) plays a significant role in a number of explanations of public scientific controversies put forward by philosophers of science.  VISS contains four items that correspond to the value-free ideal — nonsubj.1 and .2 and vfi.1 and .2.  It also contains four items corresponding to philosophical critiques of this ideal.  aims.2 corresponds to the "aims approach" to values in science, which argues that science has non-epistemic aims — such as protecting human health and the environment — and that such aims can legitimate influence all aspects of research *[cite]*.  ir corresponds to the argument from inductive risk.  According to this argument, because and insofar as science has the potential to cause harmful social consequences, it's appropriate to require greater evidence (or allow weaker evidence) based on the severity of those consequences [@DouglasSciencePolicyValuefree2009; @HavstadSensationalScienceArchaic2021].  stdpt corresponds to standpoint epistemology, a theory associated with feminist, Marxist, and critical race analyses of science *[cites]*.  According to standpoint epistemology, members of marginalized groups can be in a position to better identify and understand oppressive social phenomena than members of privileged groups.  Finally, value.conflict corresponds to an unusual convergence in two philosophical debates, one in feminist philosophy of science and the other in the relationship between science and religion.  Given that oppressive values (such as misogyny and androcentrism) have and continue to influence some lines of scientific research, some feminist philosophers — though certainly not all — argue that these values provide a sufficient grounds for feminists to question or reject the claims coming out of this research *[cites]*.  @PlantingaWhenFaithReason2001 defends a similar position in the context of debates over religion and evolution, arguing that "There is ... a battle between the Christian community and the forces of unbelief," "that important cultural forces such as science are not neutral with respect to this conflict," and so "we Christians must think about the matter at hand from a Christian perspective; we need Theistic Science" (30).  

@fig-vfi shows marginal distributions for these VFI and critiques, along with scatterplots and correlation coefficients for each VFI-critique pair.  *[marginals first, w/ xref to Likert plot: majority agreement w/ aims.2, ir, stdpt, both nonsubj, vfi.1; some rejection of value.conflict; majority rejection of vfi.2]*

While one would expect pairs of versions of VFI and critiques to be fairly strongly negatively correlated — increased acceptance of a critique would be associated with decreased acceptance of VFI — all of these correlations are near 0.  Two *[3: vfi.2 + stdpt]* pairs do reject the null hypothesis that $R \geq 0$, vfi.1 and ir (R = -0.10, p = 0.01) and nonsubj.1 and stdpt (R = -0.14, p = 0.001).  However, 33% of respondents agree (response > 4) with both vfi and ir; and 35% agree with both nonsubj.1 and stdpt.  In the discussion, we offer an interpretation of these flat correlations in terms of Deweyan "habits of thought" rather than logically inconsistent beliefs.  

![Marginal distributions and scatterplots for VFI items (rows, y-axes of scatterplots) and critiques of VFI (columns, x-axes of scatterplots).  Lines in each scatterplot are OLS regressions with 95% CIs.  Response scales are represented numerically, with 1 = strongly disagree, 7 = strongly agree.  Point positions in scatterplots are jittered for legibility.  P-values are calculated for the null that $R  \geq 0$, not adjusted for multiple comparisons.](img/02_vfi.png){#fig-vfi}

HPSTS scholars, science communicators, and scientists themselves have often claimed that the value-free ideal provides a foundation for trust in science *[cites]*.  The measures of generalized trust in science that we used here were all *[strongly]* correlated with each other; here we report results for COSS *[why]*.  @fig-trust shows scatterplots and correlation analyses between, on the one hand, the four versions of VFI and the four critiques of VFI, and on the other, COSS.  The correlations for nonsubj.2 and vfi.1 were not statistically significantly different from zero (R = -0.03, p = 0.5 for both); while correlations for nonsubj.1 and vfi.2 were significantly different from zero but were negative rather than positive (nonsubj.1: R = -0.2, p = $3 \times 10^-6$ for the point null $R = 0$; vfi.2: R = -0.2, p = $2 \times 10^-7$).  Without adjusting for multiple comparisons, all correlations for the critiques of VFI were significantly different from zero; correlations were positive for aims.2 (R = 0.2, p = $3 \times 10^-4$), stdpt (R = 0.2, p = $4 \times 10^-4$), and value.conflict (R = 0.1, p = 0.02), but negative for ir (R = -0.3, p = $4 \times 10^-10$). 

![Marginal distribution of COSS (rightmost panels) and scatterplots for VFI items (top row) and critiques of VFI (bottom row) (columns, x-axes of scatterplots) against COSS scores (y-axes). Lines in each scatterplot are OLS regressions with 95% CIs.  Response scales are represented numerically, with 1 = strongly disagree, 7 = strongly agree. Point positions in scatterplots are jittered for legibility. P-values are calculated for the null that $R = 0$, not adjusted for multiple comparisons.](img/02_trust.png){#fig-trust}
 

### Factor analysis

As in study 1, we intended to conduct an EFA and CFA of VISS responses, using the psych package (version 2.19) in R.  Because of the smaller sample size, when splitting the sample in half, the the Kaiser-Meyer-Olkin measure of sampling adequacy was 0.72, "middling."  We therefore chose to conduct only an EFA, using the whole sample.  With the whole sample, Barlett's test was significant, $\chi^2(18) = 455.74, p = 2.2 \times 10^{-16}$, and the Kaiser-Meyer-Olkin measure of sampling adequacy was slightly higher, 0.78.  

Parallel analysis suggested retaining four factors, with two factors having an eigenvalue greater than 1.0.  In the spirit of multiverse analysis [@SteegenIncreasingTransparencyMultiverse2016] we computed solutions with 1 through 6 factors using varimax rotation, analyzing all six solutions in parallel.  In order, the models explained 14% of the variance, followed by 23%, 27%, 32%, 36%, and 37%.  

Across all six models, we retained items that had at least |.3| factor loading on at least one resulting factor; loadings for all models are shown in *[table]*, grouped by interpretive label.  The models retained varying numbers of from items, from 9 (1-factor) to 18 (6-factor).  The only item that did not load onto a latent factor in any model was value.conflict, "When scientific findings conflict with my core values, it's appropriate to be especially skeptical."  *[table should also include communalities?]*

Examining the loadings for all models as once indicated consistent *[clusters]* of items.  The most consistent, occurring in some form across all six models, was the *textbook* perception of science:  uniform agreement (consensus.2), fixed (fallible), based on laboratory experiments (pluralism.1), and not relying on imagination or creativity (vfi.2).  The second most common theme was *cynicism*, which occurred distinctly in five of the six models, and also appeared (mixed with textbook items) in the one-factor model.  This theme consistently included concerns about conflicts of interest (coi.1 and coi.2), the idea that the scientific consensus is non-epistemic (consensus.1), and the idea that scientific change is an indication of error (fallible.2).  For models 2 and 3, this factor also had negative loadings for the idea that science has epistemic primary aims (aims.1) and that scientists use the "same strict requirements" for assessing hypotheses (pluralism.3).  

*[pluralism.3 ~> pluralism.2 everywhere]*

A third common theme was *objectivity*, with four versions of VFI (nonsubj.1, nonsubj.2, vfi.1, vfi.2), along with the idea that policy should wait until "the science is settled" (wait.policy).  Models 4-6 included a factor comprising three claims about the aims of science (aims.1, aims.2, aims.3) and the central claim of standpoint epistemology (stdpt).  It's not clear to us how to interpret these factors.  Models 5 and 6 also include two singleton factors, for pluralism.3 ("same strict requirements") and the argument from inductive risk (ir).  

Based on the loadings and thematic interpretation, the three-factor solution appears to be the most theoretically coherent.  

*[report any of the CFA statistics here?]*

## VISS factors and demographics


## VISS factors and trust


# General Discussion




*[Deweyan "habits of thought": vfi and crits; VFI + crits and trust]*

The patterns of correlation in @fig-trust challenge the common idea that VFI provides a foundation for trust in science.  Our findings indicate that trust in science is either independent of or *negatively* associated with VFI.   *[Further, in the study 3 EFA, the four versions of VFI and wait.policy consistently comprise the "objectivity" latent variable; and this variable is consistently uncorrelated with trust.]*  



# References



# Appendix
# The Value in Science Scale (VISS) {#sec-viss}

```{r}
#| label: tbl-viss
#| tbl-cap: "VISS items. Original item IDs and text as used in Study 1; finalized item IDs and text as refined in Study 2.  Column "modified" indicates that the text was modified in the course of Study 2."
ref = 100/7
here('out', '03', '06_prompts.Rds') |>
    read_rds() |> 
    cols_width(contains('ID') ~ pct(ref), 
               contains('text') ~ pct(2*ref), 
               modified ~ pct(ref)) |>
    opt_row_striping() |>
    tab_style(style = cell_text(weight = 'bold'), 
              locations = cells_column_labels())
```

